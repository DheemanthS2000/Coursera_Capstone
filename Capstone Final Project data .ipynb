{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final Project:- Applied data science capstone\n",
    "### Creator: Dheemanth Shenoy\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To solve the problem, we will need the following data:\n",
    "1. List of neighbourhoods in Kuala Lumpur. This defines the scope of this project which is confined to the city of Kuala Lumpur, the capital city of the country of Malaysia in South East Asia.\n",
    "1. Latitude and longitude coordinates of those neighbourhoods. This is required in order to plot the map and also to get the venue data.\n",
    "1. Venue data, particularly data related to shopping malls. We will use this data to perform clustering on the neighbourhoods.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source and Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wikipedia page ( __[ Page link ](https://en.wikipedia.org/wiki/Category:Suburbs_in_Kuala_Lumpur)__ ) contains a list of neighbourhoods in Kuala Lumpur, with a total of 70 neighbourhoods. We will use web scraping techniques to extract the data from the Wikipedia page, with the help of Python requests and beautifulsoup packages. Then we will get the geographical coordinates of the neighbourhoods using Python Geocoder package which will give us the latitude and longitude coordinates of the neighbourhoods.After that, we will use Foursquare API to get the venue data for those neighbourhoods.Foursquare has one of the largest database of 105+ million places and is used by over 125,000 developers. Foursquare API will provide many categories of the venue data, we are particularly interested in the Shopping Mall category in order to help us to solve the business problem put forward. This is a project that will make use of many data science skills, from web scraping (Wikipedia), working with API (Foursquare), data cleaning, data wrangling, to machine learning (K-means clustering) and map visualization (Folium). In the next section, we will present the Methodology section where we will discuss the steps taken in this project, the data analysis that we did and the machine learning technique that was used. \n",
    "<br>Firstly, we need to get the list of neighbourhoods in the city of Kuala Lumpur. Fortunately, the list is \n",
    "available in the Wikipedia page. \n",
    "We will do web scraping using Python requests and beautifulsoup packages to extract the list of \n",
    "neighbourhoods data. However, this is just a list of names. We need to get the geographical \n",
    "coordinates in the form of latitude and longitude in order to be able to use Foursquare API. To do so, \n",
    "we will use the wonderful Geocoder package that will allow us to convert address into geographical \n",
    "coordinates in the form of latitude and longitude. After gathering the data, we will populate the data \n",
    "into a pandas DataFrame and then visualize the neighbourhoods in a map using Folium package. This \n",
    "allows us to perform a sanity check to make sure that the geographical coordinates data returned by \n",
    "Geocoder are correctly plotted in the city of Kuala Lumpur. \n",
    "<br>Next, we will use Foursquare API to get the top 100 venues that are within a radius of 2000 meters. \n",
    "We need to register a Foursquare Developer Account in order to obtain the Foursquare ID and \n",
    "Foursquare secret key. We then make API calls to Foursquare passing in the geographical \n",
    "coordinates of the neighbourhoods in a Python loop. Foursquare will return the venue data in JSON \n",
    "format and we will extract the venue name, venue category, venue latitude and longitude. With the \n",
    "data, we can check how many venues were returned for each neighbourhood and examine how \n",
    "many unique categories can be curated from all the returned venues. <br>Then, we will analyse each \n",
    "neighbourhood by grouping the rows by neighbourhood and taking the mean of the frequency of \n",
    "occurrence of each venue category. By doing so, we are also preparing the data for use in clustering. \n",
    "Since we are analysing the “Shopping Mall” data, we will filter the “Shopping Mall” as venue \n",
    "category for the neighbourhoods. \n",
    "Lastly, we will perform clustering on the data by using k-means clustering. K-means clustering \n",
    "algorithm identifies k number of centroids, and then allocates every data point to the nearest \n",
    "cluster, while keeping the centroids as small as possible. It is one of the simplest and popular \n",
    "unsupervised machine learning algorithms and is particularly suited to solve the problem for this \n",
    "project. We will cluster the neighbourhoods into 3 clusters based on their frequency of occurrence \n",
    "for “Shopping Mall”. The results will allow us to identify which neighbourhoods have higher \n",
    "concentration of shopping malls while which neighbourhoods have fewer number of shopping malls. \n",
    "Based on the occurrence of shopping malls in different neighbourhoods, it will help us to answer the \n",
    "question as to which neighbourhoods are most suitable to open new shopping malls. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
